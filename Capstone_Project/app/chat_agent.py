from app.config.openai_client import client
from app.memory_manager import summarize_memory, load_user_memory, is_first_entry
import random

with open("debug_log.txt", "a") as f:
    f.write("âœ… chat_agent.pyê°€ FastAPIì— ë¡œë”©ë˜ì—ˆìŠµë‹ˆë‹¤!\n")

class ChatAgent:
    def __init__(self, persona="ìœ„ë¡œí˜•"):
        self.mode = "casual"
        self.intent = "ìƒë‹´ ì›í•¨"
        self.emotion = ""
        self.risk = ""
        self.persona = persona

        self.persona_prompts = {
            "ìœ„ë¡œí˜•": (
                "[í˜ë¥´ì†Œë‚˜: ìœ„ë¡œí˜•]\n"
                "ë‹¹ì‹ ì€ ë”°ëœ»í•˜ê³  ë¶€ë“œëŸ¬ìš´ ë§íˆ¬ë¥¼ ì‚¬ìš©í•˜ëŠ” ìƒë‹´ìì…ë‹ˆë‹¤.\n"
                "ê³µê°ì€ í•˜ë˜, ì„¤ëª…ì€ ìµœì†Œí™”í•˜ê³  í•µì‹¬ë§Œ ë§í•˜ì„¸ìš”.\n"
                "ì‘ë‹µì€ 1~2ë¬¸ì¥ ì´ë‚´ë¡œ ì œí•œí•˜ê³ , ê°™ì€ ë§ì„ ë°˜ë³µí•˜ì§€ ë§ˆì„¸ìš”."
            ),
            "ë…¼ë¦¬í˜•": (
                "[í˜ë¥´ì†Œë‚˜: ë…¼ë¦¬í˜•]\n"
                "ë‹¹ì‹ ì€ ì°¨ë¶„í•˜ê³  ê°ê´€ì ì¸ ì‹œì„ ìœ¼ë¡œ ìƒë‹´í•˜ëŠ” ë¶„ì„í˜• ìƒë‹´ìì…ë‹ˆë‹¤.\n"
                "ìƒí™© ì •ë¦¬ì™€ ë¬¸ì œ í•´ê²° ì¤‘ì‹¬ìœ¼ë¡œ ì´ì•¼ê¸°í•˜ì§€ë§Œ, ë§ì€ ì§§ê³  í•µì‹¬ì ì´ì–´ì•¼ í•©ë‹ˆë‹¤.\n"
                "1~2ë¬¸ì¥ ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ ë§í•˜ê³ , ìœ ì‚¬ ë¬¸ì¥ ë°˜ë³µì€ ì ˆëŒ€ í•˜ì§€ ë§ˆì„¸ìš”."
            ),
            "ê¸ì •í˜•": (
                "[í˜ë¥´ì†Œë‚˜: ê¸ì •í˜•]\n"
                "ë‹¹ì‹ ì€ ìœ ì¾Œí•˜ê³  ê¸ì •ì ì¸ ë§íˆ¬ë¥¼ ì‚¬ìš©í•˜ëŠ” ìƒë‹´ìì…ë‹ˆë‹¤.\n"
                "ì¹œê·¼í•˜ê³  ëª…ë‘í•œ ë§íˆ¬ë¡œ ì§§ê²Œ ë°˜ì‘í•˜ë˜, í•µì‹¬ì„ ë†“ì¹˜ì§€ ë§ˆì„¸ìš”.\n"
                "ì‘ë‹µì€ 1~2ë¬¸ì¥ ì´ë‚´ë¡œ ì œí•œí•˜ê³ , ê°™ì€ ìœ í˜•ì˜ ë§ì„ ë°˜ë³µí•˜ì§€ ë§ˆì„¸ìš”."
            )
        }

    def get_persona_prompt(self):
        return self.persona_prompts.get(self.persona, self.persona_prompts["ìœ„ë¡œí˜•"])

    def detect_mode_via_llm(self, user_input: str, memory: str = ""):
        prompt = f"""ì•„ë˜ ì‚¬ìš©ì ì…ë ¥ê³¼ ê³¼ê±° ëŒ€í™”ë¥¼ ë³´ê³ , ìƒë‹´ ë‹¨ê³„(casual, explore, counseling), ê°ì • í‚¤ì›Œë“œ, ìœ„í—˜ë„, ìƒë‹´ ì˜ë„ë¥¼ íŒë‹¨í•´ì£¼ì„¸ìš”.

[ê³¼ê±° ëŒ€í™”]
{memory}

[ì‚¬ìš©ì ì…ë ¥]
{user_input}

[ì‘ë‹µ í˜•ì‹ ì˜ˆì‹œ]
ë‹¨ê³„: counseling
ê°ì •: ë¬´ê¸°ë ¥, ë¶ˆì•ˆ
ìœ„í—˜ë„: ì¤‘ê°„
ì˜ë„: ìƒë‹´ ì›í•¨
"""
        try:
            result = client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": "ë„ˆëŠ” ì‹¬ë¦¬ìƒë‹´ ëŒ€í™” íë¦„ì„ ë¶„ì„í•˜ëŠ” ì¡°ë ¥ìì•¼."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.2,
                max_tokens=150
            )
            content = result.choices[0].message.content.strip()
            for line in content.splitlines():
                if "ë‹¨ê³„:" in line:
                    self.mode = line.split(":")[-1].strip().lower()
                elif "ì˜ë„:" in line:
                    self.intent = line.split(":")[-1].strip()
                elif "ê°ì •:" in line:
                    self.emotion = line.split(":")[-1].strip()
                elif "ìœ„í—˜ë„:" in line:
                    self.risk = line.split(":")[-1].strip()

            with open("debug_log.txt", "a") as f:
                f.write(f"ğŸ” ê°ì • ë¶„ì„ ê²°ê³¼ - mode: {self.mode}, emotion: {self.emotion}, risk: {self.risk}\n")

        except Exception as e:
            with open("debug_log.txt", "a") as f:
                f.write(f"[âš ï¸ ëª¨ë“œ ì˜ˆì¸¡ ì‹¤íŒ¨] {e}\n")

    def build_prompt(self, user_input: str, memory: str = "", theory: str = "") -> str:
        base_prompt = self.get_persona_prompt()

        if self.emotion:
            base_prompt += (
                f"\n\n[í˜„ì¬ ê°ì • ìƒíƒœ]\n"
                f"ì‚¬ìš©ìëŠ” '{self.emotion}'ë¼ëŠ” ê°ì •ì„ í‘œí˜„í–ˆìŠµë‹ˆë‹¤. ì´ ê°ì •ì— ëŒ€í•´ ì§„ì‹¬ ì–´ë¦° ê³µê° + ì§§ì€ ì§ˆë¬¸ì„ í¬í•¨í•˜ì„¸ìš”."
            )

        if self.risk.lower() in ["ì¤‘ê°„", "ë†’ìŒ"]:
            base_prompt += "\nìœ„í—˜ë„ê°€ ë†’ìœ¼ë¯€ë¡œ ì¡°ì‹¬ìŠ¤ëŸ½ê³  ê°„ê²°í•˜ê²Œ í‘œí˜„í•´ì£¼ì„¸ìš”."

        core_instruction = (
            "\n\nğŸ§  ë‹¹ì‹ ì€ ì§„ì§œ ì‚¬ëŒì²˜ëŸ¼ ë”°ëœ»í•˜ê³  ê³µê°ë ¥ ìˆëŠ” ì „ë¬¸ ì‹¬ë¦¬ìƒë‹´ìì…ë‹ˆë‹¤.\n"
            "ìƒë‹´ì€ ì§„ì‹¬ ì–´ë¦° ê³µê°ìœ¼ë¡œ ì‹œì‘ë˜ê³ , ê°ì •ì— ì •í™•íˆ ë°˜ì‘í•˜ë©°, ë‹¤ìŒ ë§ì„ ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ê°€ì•¼ í•©ë‹ˆë‹¤.\n"
            "ì‚¬ìš©ìì˜ ë§ ì†ì—ì„œ ê°ì •, ìƒí™©, ìš•êµ¬ë¥¼ íŒŒì•…í•˜ê³ , ë‹¤ìŒ ë‘ ê°€ì§€ë¥¼ ì¡°í•©í•˜ì—¬ ì‘ë‹µí•˜ì„¸ìš”:\n"
            "- (1) ê°ì •ì— ëŒ€í•œ ê³µê° í‘œí˜„\n"
            "- (2) ê°ì •ì„ ìœ ë„í•˜ê±°ë‚˜, ì¡°ê¸ˆ ë” ê¹Šì´ ë¬¼ì–´ë³¼ ìˆ˜ ìˆëŠ” ì§§ì€ ì§ˆë¬¸\n"
            "ì ˆëŒ€ íŒë‹¨í•˜ì§€ ë§ê³ , ì„¤ëª…ë„ ìµœì†Œí™”í•˜ë©°, ë§ì€ ì§§ê³  ì§„ì‹¬ ìˆê²Œ í•´ì•¼ í•©ë‹ˆë‹¤.\n"
            "í˜ë¥´ì†Œë‚˜ì— ë”°ë¼ ë§íˆ¬ë§Œ ë‹¬ë¼ì§€ë©°, ì§„ì‹¬ê³¼ íë¦„ì€ ëª¨ë‘ ë™ì¼í•©ë‹ˆë‹¤.\n"
            "ë°˜ë“œì‹œ 2ë¬¸ì¥ì„ ë„˜ê¸°ì§€ ë§ˆì„¸ìš”."
        )

        return f"{base_prompt}\n{core_instruction}\n\n[ëŒ€í™” íë¦„ ìš”ì•½]\n{memory}\n\n[ìƒë‹´ ì´ë¡  ìš”ì•½]\n{theory}\n\n[ì‚¬ìš©ì ë§]\n{user_input}"

    def respond(self, user_input: str, message_log: list, member_id: str, theory: list = None, max_tokens: int = 400) -> str:
        with open("debug_log.txt", "a") as f:
            f.write(f"\nğŸ§© respond ì§„ì… | user_input: {user_input}\n")

        if is_first_entry(member_id, message_log):
            return "ì•ˆë…•í•˜ì„¸ìš”! ì²˜ìŒ ì˜¤ì…¨êµ°ìš”. í¸í•˜ê²Œ ì´ì•¼ê¸°í•´ ì£¼ì„¸ìš”. ğŸ˜Š"

        memory_raw = load_user_memory(member_id, message_log)
        memory = summarize_memory(memory_raw)

        if len(user_input) > 10:
            self.detect_mode_via_llm(user_input, memory)

        theory_text = "\n".join([f"[{name}] {desc}" for name, desc in theory]) if isinstance(theory, list) else theory or ""
        system_prompt = self.build_prompt(user_input, memory, theory_text)

        with open("debug_log.txt", "a") as f:
            f.write("ğŸ§  build_prompt ì™„ë£Œ. GPT í˜¸ì¶œ ì‹œì‘...\n")

        try:
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": f"(ê³¼ê±° ëŒ€í™”)\n{memory}"},
                {"role": "user", "content": user_input}
            ]

            response = client.chat.completions.create(
                model="gpt-4o",
                messages=messages,
                temperature=0.4 if self.mode == "counseling" else 0.7,
                max_tokens=max_tokens
            )
            reply = response.choices[0].message.content.strip().replace('\n', ' ')

            with open("debug_log.txt", "a") as f:
                f.write(f"âœ… GPT ì‘ë‹µ ìˆ˜ì‹ : {reply}\n")

            with open("debug_log.txt", "a") as f:
                f.write(f"ğŸ“Œ ì‚¬ìš©ëœ ëª¨ë¸: {response.model}\\n")

            fallback_candidates = [
                "ì§€ê¸ˆ ë§í•´ì£¼ì‹  ê²ƒë§Œìœ¼ë¡œë„ ì¶©ë¶„íˆ ì†Œì¤‘í•´ìš”. í˜¹ì‹œ ë” ë‚˜ëˆ ì£¼ì‹¤ ìˆ˜ ìˆì„ê¹Œìš”?",
                "ë§ˆìŒì´ ë³µì¡í•˜ì…¨ê² ì–´ìš”. í¸í•˜ì‹¤ ë•Œ ì²œì²œíˆ ì´ì–´ì„œ ë§í•´ì£¼ì…”ë„ ê´œì°®ì•„ìš”.",
                "ì˜ ì „ë‹¬ë˜ì—ˆì–´ìš”. ì–´ë–¤ ë¶€ë¶„ë¶€í„° ì´ì•¼ê¸°í•˜ê³  ì‹¶ì€ì§€ ì•Œë ¤ì£¼ì‹¤ë˜ìš”?"
            ]

            if (
                len(reply) < 15 or
                any(x in reply.lower() for x in ["ì˜ ëª¨ë¥´ê² ì–´ìš”", "ì£„ì†¡", "ì–´ë ¤ì›Œìš”", "í™•ì‹¤í•˜ì§€ ì•Šì•„ìš”"])
            ):
                with open("debug_log.txt", "a") as f:
                    f.write("ğŸ§© ì‘ë‹µ í’ˆì§ˆ ë‚®ìŒ - fallback ë¬¸êµ¬ ë°˜í™˜\n")
                return random.choice(fallback_candidates)

            return reply

        except Exception as e:
            with open("debug_log.txt", "a") as f:
                f.write(f"âš ï¸ GPT í˜¸ì¶œ ì‹¤íŒ¨: {e}\n")
            return "ì¡°ê¸ˆ ë” êµ¬ì²´ì ìœ¼ë¡œ ì´ì•¼ê¸°í•´ì£¼ì‹¤ ìˆ˜ ìˆì„ê¹Œìš”?"