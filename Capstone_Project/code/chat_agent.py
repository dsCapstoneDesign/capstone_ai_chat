from config.openai_client import client  # âœ… ê³µí†µ client import

class ChatAgent:
    def __init__(self, persona="ìœ„ë¡œí˜•"):
        self.turn = 0
        self.mode = "casual"
        self.intent = "ìƒë‹´ ì›í•¨"
        self.emotion = ""   # ğŸ”„ ê°ì • ìƒíƒœ ì €ì¥
        self.risk = ""      # ğŸ”„ ìœ„í—˜ë„ ì €ì¥
        self.persona = persona

        self.persona_prompts = {
            "ìœ„ë¡œí˜•": (
                "ë‹¹ì‹ ì€ ë”°ëœ»í•˜ê³  ë¶€ë“œëŸ¬ìš´ ë§íˆ¬ë¥¼ ì‚¬ìš©í•˜ëŠ” ìƒë‹´ìì…ë‹ˆë‹¤.\n"
                "ì—„ë§ˆì²˜ëŸ¼ í¬ê·¼í•˜ê²Œ ì‚¬ìš©ìì˜ ê°ì •ì„ ê°ì‹¸ì¤ë‹ˆë‹¤.\n"
                "ê¸°ë¶„ì´ ì–´ë–¤ì§€ ìì—°ìŠ¤ëŸ½ê²Œ ë¬¼ì–´ë³´ë©° ëŒ€í™”ë¥¼ ì‹œì‘í•˜ê³ ,\n"
                "í˜ë“  ì´ì•¼ê¸°ë¥¼ í„¸ì–´ë†“ì„ ìˆ˜ ìˆë„ë¡ í¸ì•ˆí•œ ë¶„ìœ„ê¸°ë¥¼ ë§Œë“¤ì–´ì¤ë‹ˆë‹¤.\n"
                "ì¡´ëŒ“ë§ì„ ì‚¬ìš©í•˜ë©°, íŒë‹¨í•˜ì§€ ì•Šê³  ì¡°ìš©íˆ ë“¤ì–´ì£¼ëŠ” ìì„¸ë¥¼ ìœ ì§€í•©ë‹ˆë‹¤."
            ),
            "ë…¼ë¦¬ë¶„ì„í˜•": (
                "ë‹¹ì‹ ì€ ì°¨ë¶„í•˜ê³  ê°ê´€ì ì¸ ì‹œì„ ìœ¼ë¡œ ìƒë‹´í•˜ëŠ” ë¶„ì„ ì¤‘ì‹¬ ìƒë‹´ìì…ë‹ˆë‹¤.\n"
                "ê¸°ë¶„ì´ë‚˜ ìƒê°ì„ ë¨¼ì € ë¬¼ì–´ë³´ë©° ì ‘ê·¼í•˜ê³ ,\n"
                "ì‚¬ìš©ìì˜ ê³ ë¯¼ì„ ë‹¨ê³„ì ìœ¼ë¡œ ì •ë¦¬í•˜ê³ , ì›ì¸ì„ íŒŒì•…í•´ ë¬¸ì œ í•´ê²°ì„ ë„ì™€ì¤ë‹ˆë‹¤.\n"
                "ê°ì •ë³´ë‹¤ëŠ” ìƒí™© ë¶„ì„, í–‰ë™ íŒ¨í„´, ë…¼ë¦¬ì  êµ¬ì¡°ì— ì§‘ì¤‘í•©ë‹ˆë‹¤."
            ),
            "ìœ ì¾Œí•œì¹œêµ¬í˜•": (
                "ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì¹œí•œ ì¹œêµ¬ì²˜ëŸ¼ í¸í•˜ê²Œ ì´ì•¼ê¸°í•˜ëŠ” ìƒë‹´ìì…ë‹ˆë‹¤.\n"
                "ì²˜ìŒì—” 'ê¸°ë¶„ ì–´ë•Œ?'ì²˜ëŸ¼ ë¼ì´íŠ¸í•˜ê²Œ ì‹œì‘í•˜ê³ ,\n"
                "ë°˜ë§ê³¼ ì´ëª¨ì§€ë¡œ ì¹œê·¼í•˜ê³  ìœ ì¾Œí•˜ê²Œ ë°˜ì‘í•©ë‹ˆë‹¤.\n"
                "í•„ìš” ì‹œ ì§„ì§€í•œ ê³µê°ë„ í•¨ê»˜ ì œê³µí•©ë‹ˆë‹¤."
            ),
            "ì—¬ìì¹œêµ¬í˜•": (
                "ë‹¹ì‹ ì€ ê°ì„±ì ì´ê³  ë”°ëœ»í•œ ì—¬ìì¹œêµ¬ì²˜ëŸ¼ ëŒ€í™”í•˜ëŠ” ìƒë‹´ìì…ë‹ˆë‹¤.\n"
                "'ìê¸°ì•¼, ì˜¤ëŠ˜ í•˜ë£¨ ì–´ë• ì–´?'ì²˜ëŸ¼ ë§í•˜ë©° ë‹¤ì •í•˜ê²Œ ì‹œì‘í•©ë‹ˆë‹¤.\n"
                "ê³µê°ê³¼ ì¹­ì°¬ì„ ì„ì–´ ì •ì„œì  ì•ˆì •ê°ì„ ì¤ë‹ˆë‹¤."
            ),
            "ë‚¨ìì¹œêµ¬í˜•": (
                "ë‹¹ì‹ ì€ ë“¬ì§í•˜ê³  ë¶€ë“œëŸ¬ìš´ ë‚¨ìì¹œêµ¬ì²˜ëŸ¼ ë§í•˜ëŠ” ìƒë‹´ìì…ë‹ˆë‹¤.\n"
                "ì²˜ìŒì—” ê°€ë³ê²Œ ê¸°ë¶„ì„ ë¬»ê³ , ì²œì²œíˆ ì´ì•¼ê¸°ë¥¼ ë“¤ì–´ì¤ë‹ˆë‹¤.\n"
                "ì¡´ì¤‘ê³¼ ê²©ë ¤ë¥¼ ë‹´ì•„ ìœ„ë¡œí•˜ê³ , ìì‹ ê°ì„ ì‹¬ì–´ì¤ë‹ˆë‹¤."
            )
        }

    def get_persona_prompt(self):
        return self.persona_prompts.get(self.persona, self.persona_prompts["ìœ„ë¡œí˜•"])

    def detect_mode_via_llm(self, user_input: str, memory: str = ""):
        prompt = f"""ì•„ë˜ ì‚¬ìš©ì ì…ë ¥ê³¼ ê³¼ê±° ëŒ€í™”ë¥¼ ë³´ê³ , ìƒë‹´ ë‹¨ê³„(casual, explore, counseling), ê°ì • í‚¤ì›Œë“œ, ìœ„í—˜ë„, ìƒë‹´ ì˜ë„ë¥¼ íŒë‹¨í•´ì£¼ì„¸ìš”.

[ê³¼ê±° ëŒ€í™”]
{memory}

[ì‚¬ìš©ì ì…ë ¥]
{user_input}

[ì‘ë‹µ í˜•ì‹ ì˜ˆì‹œ]
ë‹¨ê³„: counseling
ê°ì •: ë¬´ê¸°ë ¥, ë¶ˆì•ˆ
ìœ„í—˜ë„: ì¤‘ê°„
ì˜ë„: ìƒë‹´ ì›í•¨
"""
        try:
            result = client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": "ë„ˆëŠ” ì‹¬ë¦¬ìƒë‹´ ëŒ€í™” íë¦„ì„ ë¶„ì„í•˜ëŠ” ì¡°ë ¥ìì•¼."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.2,
                max_tokens=150
            )
            content = result.choices[0].message.content.strip()
            for line in content.splitlines():
                if "ë‹¨ê³„:" in line:
                    self.mode = line.split(":")[-1].strip().lower()
                if "ì˜ë„:" in line:
                    self.intent = line.split(":")[-1].strip()
                if "ê°ì •:" in line:
                    self.emotion = line.split(":")[-1].strip()
                if "ìœ„í—˜ë„:" in line:
                    self.risk = line.split(":")[-1].strip()
        except Exception as e:
            print(f"[âš ï¸ ëª¨ë“œ ì˜ˆì¸¡ ì‹¤íŒ¨] {e} â†’ ê¸°ì¡´ ëª¨ë“œ ìœ ì§€: {self.mode}, {self.intent}")

    def build_prompt(self, user_input: str, memory: str = "", theory: str = "") -> str:
        base_prompt = self.get_persona_prompt()

        if self.emotion:
            base_prompt += f"\n\ní˜„ì¬ ì‚¬ìš©ìì˜ ê°ì • ìƒíƒœëŠ” '{self.emotion}'ì…ë‹ˆë‹¤. ì´ì— ê³µê°í•˜ë©° ìì—°ìŠ¤ëŸ½ê²Œ ë°˜ì‘í•´ì£¼ì„¸ìš”."
        if self.risk.lower() in ["ì¤‘ê°„", "ë†’ìŒ"]:
            base_prompt += "\n\nìƒëŒ€ë°©ì´ ë¯¼ê°í•˜ê±°ë‚˜ í˜ë“  ìƒíƒœì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ë” ì‹ ì¤‘í•˜ê³  ì¡°ì‹¬ìŠ¤ëŸ½ê²Œ ë§í•´ì£¼ì„¸ìš”."

        if memory and self.turn == 0:
            base_prompt += f"\n\nì´ì „ì— ì‚¬ìš©ìì™€ ë‹¤ìŒê³¼ ê°™ì€ ëŒ€í™”ë¥¼ ë‚˜ëˆˆ ì ì´ ìˆìŠµë‹ˆë‹¤:\n{memory}\nê·¸ ë‚´ìš©ì„ ë– ì˜¬ë¦¬ë©° ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ì„œ ëŒ€í™”ë¥¼ ì‹œì‘í•´ì£¼ì„¸ìš”.\nì˜ˆ: 'ì €ë²ˆì—” í”„ë¡œì íŠ¸ê°€ í˜ë“¤ë‹¤ê³  í•˜ì…¨ì£ . ìš”ì¦˜ì€ ì¢€ ë‚˜ì•„ì§€ì…¨ë‚˜ìš”?'"

        if self.turn >= 3:
            base_prompt += "\n\nì´ë¯¸ ëª‡ ì°¨ë¡€ ìƒë‹´ì´ ì§„í–‰ë˜ì—ˆìœ¼ë¯€ë¡œ, ê³µê°ì„ ë„˜ì–´ ì‹¤ì§ˆì ì¸ ì œì•ˆì´ë‚˜ ì „ëµë„ ì œê³µí•´ì£¼ì„¸ìš”."
        if self.turn >= 6:
            base_prompt += "\n\nì‚¬ìš©ìê°€ ì¶©ë¶„íˆ ì‹ ë¢°ë¥¼ ë³´ì´ê³  ìˆìœ¼ë¯€ë¡œ, ë³´ë‹¤ ì ê·¹ì ì´ê³  êµ¬ì²´ì ì¸ í–‰ë™ ì œì•ˆë„ í¬í•¨í•´ì£¼ì„¸ìš”."

        theory_instruction = ""
        if self.intent == "ìƒë‹´ ì›í•¨" and theory:
            theory_instruction = (
                "\n\nìƒë‹´ ì´ë¡ ì„ ë‹¨ìˆœíˆ ì„¤ëª…í•˜ì§€ ë§ê³ , ì‚¬ìš©ìì˜ ìƒí™©ì— ì ìš©í•´ì£¼ì„¸ìš”. "
                "í–‰ë™ ì˜ˆì‹œë‚˜ êµ¬ì²´ì ì¸ ì¡°ì–¸ì„ í¬í•¨í•˜ê³ , ì‹¤ì œë¡œ ë„ì›€ì´ ë  ìˆ˜ ìˆê²Œ ë§í•´ì£¼ì„¸ìš”."
            )

        closing_instruction = "\n\nì‘ë‹µì€ ë¶€ë“œëŸ½ê³  ë”°ëœ»í•˜ê²Œ ë§ˆë¬´ë¦¬í•´ì£¼ì„¸ìš”. ë¬¸ì¥ì´ ëë‚¬ë‹¤ëŠ” ëŠë‚Œì´ ë“¤ë„ë¡ ìì—°ìŠ¤ëŸ½ê²Œ ë§ˆê°í•˜ê³ , ë‹¤ìŒ ëŒ€í™”ë¥¼ ìœ ë„í•˜ëŠ” ì§ˆë¬¸ìœ¼ë¡œ ë§ˆì¹˜ë©´ ì¢‹ì•„ìš”."

        return f"""{base_prompt}{theory_instruction}{closing_instruction}

[ê³¼ê±° ëŒ€í™” ìš”ì•½]
{memory}

[ìƒë‹´ ì´ë¡  ìš”ì•½]
{theory}

[ì‚¬ìš©ì ì…ë ¥]
{user_input}

[ìƒë‹´ì ì‘ë‹µ]
""".strip()

    def respond(self, user_input: str, memory: str = "", theory: list = None, max_tokens: int = 300) -> str:
        self.turn += 1
        self.detect_mode_via_llm(user_input, memory)

        # âœ… theoryê°€ (ì´ë¡ ëª…, ì„¤ëª…) íŠœí”Œ ë¦¬ìŠ¤íŠ¸ì¼ ê²½ìš° ì²˜ë¦¬
        if isinstance(theory, list) and theory and isinstance(theory[0], tuple):
            theory_text = "\n".join([f"[{name}] {desc}" for name, desc in theory])
        else:
            theory_text = theory or ""

        prompt = self.build_prompt(user_input, memory, theory_text)

        try:
            response = client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": self.get_persona_prompt()},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.4 if self.mode == "counseling" else 0.7,
                max_tokens=max_tokens
            )
            reply = response.choices[0].message.content.strip()

            if (
                len(reply) < 30 or
                any(x in reply.lower() for x in ["ì˜ ëª¨ë¥´ê² ì–´ìš”", "ì£„ì†¡", "ê·¸ê±´ ì–´ë ¤ì›Œìš”", "í™•ì‹¤í•˜ì§€ ì•Šì•„ìš”"])
            ):
                return "ì¡°ê¸ˆ ë” êµ¬ì²´ì ìœ¼ë¡œ ì´ì•¼ê¸°í•´ì£¼ì‹¤ ìˆ˜ ìˆì„ê¹Œìš”? ë‹¹ì‹ ì˜ ì´ì•¼ê¸°ë¥¼ ë“£ê³  ì‹¶ì–´ìš”."

            return reply

        except Exception as e:
            return f"[âš ï¸ OpenAI ì‘ë‹µ ì‹¤íŒ¨] {e}"
